# ML-and-Deep-Learning-Notebooks

This repository contains a collection of notebooks that were proposed as part of my *Mathematical Foundations of Machine Learning* course at Imperial College London.  
I have completed and adapted these notebooks to reinforce my understanding of core machine learning and deep learning concepts.  
They illustrate the models, techniques, and optimisation methods that I currently master.

The notebooks cover a range of topics, from classical neural network architectures to modern transformer-based models and optimisation methods.

---

## Content : 

### **1.TutorialPytorch.ipynb**
A practical introduction to PyTorch.  
Includes tensors, autograd, custom modules, training loops, and dataset handling.

### **2. OptimisationGradientDescent.ipynb**
Exploration of optimisation techniques including gradient descent, learning rate effects, convergence behaviours, and visualisation of loss landscapes.

### **3. NeuralNetworks.ipynb**
Covers the fundamentals of feedforward neural networks, activation functions, loss computation, and backpropagation.

### **4. CNN.ipynb**
Introduction to Convolutional Neural Networks, including convolutional layers, pooling, and training a basic classifier on a standard dataset.

### **5. GaussianProcessesRegression.ipynb**
Introduction to Gaussian Process Regression (GPR), kernel functions, and probabilistic predictions.

### **6. Auto-Encoder.ipynb**
Implementation of a simple autoencoder for dimensionality reduction and data reconstruction.  
Explores encoderâ€“decoder architectures and reconstruction loss.

### **7. ViT.ipynb**
A study notebook implementing a simplified Vision Transformer (ViT).  
Focuses on patch embeddings, multi-head self-attention, and training dynamics.




